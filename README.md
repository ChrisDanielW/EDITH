# EDITH - Even Disconnected, I'm The Helper ğŸ‘“

<div align="center">

**A modern, AI study assistant that runs completely offline**

*Smart conversation management â€¢ Context-aware responses â€¢ Minimalist web UI*

![EDITH Landing Page](images/Screenshot%202025-11-09%20222627.png)

</div>

---

## ğŸŒŸ What is EDITH?

EDITH is your personal AI study assistant that helps you make sense of your notes using local LLMs. She features a modern interface with conversation management, intelligent query classification, and context-aware responses. Best of all? She runs **completely offline** using LLaMA 3.1.

![EDITH Chat Interface](images/Screenshot%202025-11-09%20223010.png)

### Key Features

âœ¨ **Modern Web Interface**
- Modern landing page with welcoming design
- Conversation management (create, save, switch, delete)
- Clean, animated UI with expandable sidebar
- Real-time typing indicators and status updates

ğŸ§  **Intelligent AI Assistant**
- Context-aware responses that reference previous messages
- Automatic classification between knowledge queries and casual chat
- RAG (Retrieval-Augmented Generation) for note-based answers
- Conversational mode for general questions

ğŸ“š **Powerful Note Processing**
- Multi-format support (PDF, DOCX, images with OCR, text files)
- Drag-and-drop or multi-file upload
- Automatic text chunking and embedding generation
- Vector database storage with Pinecone for fast retrieval

ğŸ”’ **Privacy First**
- 100% local LLM execution via Ollama
- No data sent to external servers
- Your notes stay on your machine

---

## ğŸš€ Quick Start

### 1. Install Ollama & Pull Model

```powershell
# Download Ollama from: https://ollama.ai/download
# Then pull LLaMA 3.1:
ollama pull llama3.1:8b-instruct-q4_K_M
```

### 2. Install Dependencies

```powershell
# Clone the repository
git clone https://github.com/ChrisDanielW/EDITH.git
cd EDITH

# Install Python dependencies
pip install -r requirements.txt
```

### 3. Configure Pinecone

```powershell
# Copy environment file
cp .env.example .env

# Edit .env and add your Pinecone API key
notepad .env
```

Get a free Pinecone API key at [pinecone.io](https://www.pinecone.io/)

### 4. Start EDITH

```powershell
# Start the web UI and API server
python start_ui.py
```

Open your browser to **http://localhost:5000** and start chatting!

---

## ğŸ“– User Guide

### First Time Setup

1. **Upload Your Notes**
   - Click the ğŸ“ Upload button in the sidebar
   - Select or drag-and-drop your documents (PDF, DOCX, TXT, images)
   - EDITH will process and index them automatically

![Upload Interface](images/Screenshot%202025-11-09%20223330.png)

2. **Start a Conversation**
   - Type your first message on the landing page
   - A new numbered conversation will be created automatically
   - Ask questions about your notes or just chat casually

### Using EDITH

**Asking About Notes:**
```
You: What is polymorphism in OOP?
EDITH: [Searches your notes and provides detailed explanation]
```

**Casual Conversation:**
```
You: Hey, how's it going?
EDITH: [Responds naturally without searching notes]
```

**Follow-up Questions:**
```
You: Can you explain that in more detail?
EDITH: [References previous conversation context]
```

![Conversation with Context](images/Screenshot%202025-11-09%20223314.png)

### Managing Conversations

- **New Conversation**: Click the â• button (appears when in a conversation)
- **Switch Conversations**: Click any conversation in the left sidebar
- **Delete Conversation**: Click the Ã— button on any conversation
- **Return to Landing**: Click the hamburger menu (â˜°) to collapse sidebar

---

## ğŸ—ï¸ Architecture

### Tech Stack

**Frontend:**
- Vanilla HTML, CSS, JavaScript
- LocalStorage for conversation persistence
- Modern animated UI with responsive design

**Backend:**
- Flask REST API
- Python 3.8+
- Ollama for LLM execution

**AI/ML:**
- LLaMA 3.1 (8B Instruct, 4-bit quantized)
- Sentence Transformers for embeddings
- Pinecone vector database
- RAG architecture for context retrieval

### How It Works

1. **Document Upload** â†’ Text extraction & chunking â†’ Embedding generation â†’ Store in Pinecone
2. **User Query** â†’ Classify (knowledge vs. casual) â†’ Retrieve relevant chunks (if knowledge) â†’ Generate answer with conversation context
3. **Conversation History** â†’ Last 3 exchanges sent with each query â†’ Context-aware responses

---

## ğŸ“ Project Structure

```
EDITH/
â”œâ”€â”€ ui/                          # Web interface
â”‚   â”œâ”€â”€ index.html              # Main HTML
â”‚   â”œâ”€â”€ styles.css              # Styling
â”‚   â””â”€â”€ app.js                  # Frontend logic
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Core EDITH class
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ app.py              # Flask API server
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ llama_client.py     # LLM interface
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ rag_service.py      # RAG pipeline
â”‚   â”‚   â”œâ”€â”€ vector_store.py     # Pinecone integration
â”‚   â”‚   â”œâ”€â”€ note_analyzer.py    # Document analysis
â”‚   â”‚   â””â”€â”€ summarizer.py       # Summarization
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ document_loader.py  # File loading
â”‚   â”‚   â”œâ”€â”€ text_chunker.py     # Smart chunking
â”‚   â”‚   â”œâ”€â”€ embeddings.py       # Embedding generation
â”‚   â”‚   â””â”€â”€ query_classifier.py # Query classification
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ settings.py         # Configuration
â”œâ”€â”€ start_ui.py                 # Launch script
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

---

## âš™ï¸ Configuration

### Token Limits (Adjustable in `src/main.py`)

- **RAG Mode**: 500 tokens (detailed educational responses)
- **Conversational Mode**: 350 tokens (natural chat)
- **Fallback Mode**: 400 tokens (general knowledge)

### Model Selection

Edit `src/config/settings.py` to change models:

```python
# Current default
MODEL_NAME = "llama3.1:8b-instruct-q4_K_M"

# For more powerful responses (slower, needs more RAM)
MODEL_NAME = "llama3.1:70b-instruct-q4_K_M"
```

### Vector Database

EDITH uses Pinecone with these settings:
- **Top K**: 3 most relevant chunks
- **Similarity Threshold**: 0.7
- **Max Context**: 2000 characters

---

## ğŸ¨ Features Deep Dive

### Conversation Management
- Persistent storage in browser localStorage
- Numbered conversations (1, 2, 3...)
- Auto-save after every message
- Landing page shows on startup

### Intelligent Query Routing
- Automatic classification of user intent
- Knowledge queries â†’ RAG mode (searches notes)
- Casual queries â†’ Conversational mode (direct chat)
- Hybrid queries â†’ RAG with conversational tone

### Context Awareness
- Sends last 6 messages (3 exchanges) with each query
- References previous conversation naturally
- Maintains conversation flow across messages

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- [ ] Export conversations to PDF/text
- [ ] Search within conversations
- [ ] Custom system prompts per conversation
- [ ] Markdown rendering in responses
- [ ] Code syntax highlighting
- [ ] Voice input/output

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details

---

## ğŸ™ Acknowledgments

- **LLaMA 3.1** by Meta AI
- **Ollama** for easy local LLM deployment
- **Pinecone** for vector database
- **Sentence Transformers** for embeddings

---

<div align="center">

Made with â¤ï¸ for students who want to study smarter

**[Report Bug](https://github.com/ChrisDanielW/EDITH/issues)** â€¢ **[Request Feature](https://github.com/ChrisDanielW/EDITH/issues)**

</div>